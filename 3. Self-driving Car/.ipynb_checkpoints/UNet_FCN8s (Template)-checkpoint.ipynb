{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVxGqaMqppnt"
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvM8hwMIproK",
    "outputId": "756308a8-203b-40e7-a2a9-b4815d8e4ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cNmPnRXqFT_"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"your data path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfwctuSTpnIy"
   },
   "source": [
    "## Import some Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4Rwt1IEZokg6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxtL9QB1qfyF"
   },
   "source": [
    "## Datasets Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIKDzX13qYUC"
   },
   "outputs": [],
   "source": [
    "class CityScapes(torch.utils.data.DataLoader):\n",
    "  def __init__(self, image_folder_path, mask_folder_path):\n",
    "    pass\n",
    "\n",
    "  def __len__(self):\n",
    "    pass\n",
    "\n",
    "  def __getitem(self, index):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQhZ2-u3q1-p"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBovn13orATR"
   },
   "source": [
    "- DoubleConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwUgd0T0q_w5"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8UbChejq4KB"
   },
   "source": [
    "- Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2xJJ2qpq58l"
   },
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # doblemconv, dobleconv..\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        #upsam, doubleconv, up, ..\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(#0\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))#1\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1] # reversed(skip_connections)\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgnUYnAZrDwM"
   },
   "source": [
    "# FCN8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3ez7Oc8rDEX"
   },
   "outputs": [],
   "source": [
    "class FCN8s(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, features= [64, 128, 256, 512, 1024]):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    for feature in features:\n",
    "      self.layers.append(DoubleConv(in_channels, feature))\n",
    "      in_channels = feature\n",
    "\n",
    "    self.ups1 = nn.ConvTranspose2d(features[-1], features[-2], kernel_size=2, stride=2)\n",
    "    self.ups2 = nn.ConvTranspose2d(features[-1], features[-3], kernel_size=2, stride=2)\n",
    "\n",
    "    self.predictions = nn.ConvTranspose2d(features[-2], out_channels, kernel_size=8, stride=8)\n",
    "\n",
    "  def forward(self, x):\n",
    "    skip_connections=[]\n",
    "\n",
    "    for idx,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = self.pool(x)\n",
    "      if idx in [2,3]:\n",
    "        skip_connections.append(x)\n",
    "\n",
    "\n",
    "    ups1 = self.ups1(x)\n",
    "    concat1 = torch.concat([ups1, skip_connections[-1]], dim=1)\n",
    "\n",
    "    ups2 = self.ups2(concat1)\n",
    "    concat2 = torch.concat([ups2, skip_connections[-2]], dim=1)\n",
    "\n",
    "    return self.predictions(concat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUYBnO-iq0SW"
   },
   "source": [
    "## Engine function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFRJ2NbgsoLl"
   },
   "source": [
    "- Dice coefficient metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5zHKY1UsnuK"
   },
   "outputs": [],
   "source": [
    "def calculate_dice_coefficient(ground_truth, predicted):\n",
    "    intersection = np.logical_and(ground_truth, predicted)\n",
    "    dice_coefficient = (2 * np.sum(intersection)) / (np.sum(ground_truth) + np.sum(predicted))\n",
    "    return dice_coefficient\n",
    "\n",
    "def calculate_dice_coefficients(ground_truths, predictions):\n",
    "    num_samples = len(ground_truths)\n",
    "    dice_coefficients = np.zeros(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        dice_coefficients[i] = calculate_dice_coefficient(ground_truths[i], predictions[i])\n",
    "    return dice_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Troo9nfjssci"
   },
   "source": [
    "- training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_oZFYpBqxTA"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "  #please do training step in this function\n",
    "  pass\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "  #please do evaluation step that calculate evaluation loss and evaluation metrics dice_score_coefficient\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7usHljffrTA_"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  #please init everthing in here and do the training process\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aS2CkUesbqq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
